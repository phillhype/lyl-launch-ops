import os, sys, json, time, re
from datetime import datetime
import requests
from dotenv import load_dotenv

load_dotenv()

API = "https://api.clickup.com/api/v2"
TOKEN = os.getenv("CLICKUP_TOKEN")
TEAM  = os.getenv("CLICKUP_TEAM")
SPACE_ID = os.getenv("CLICKUP_SPACE_ID")  # Opcional: se definido, pula busca por nome

if not TOKEN or not TEAM:
    print("Erro: defina CLICKUP_TOKEN e CLICKUP_TEAM no .env")
    sys.exit(1)

HEAD = {"Authorization": TOKEN, "Content-Type": "application/json"}

# Mapeamento de √°reas can√¥nicas para nomes conhecidos das listas antigas
AREA_SYNONYMS = {
    "projetos": ["Planejamentos & Cronogramas", "Planejamentos", "Cronogramas", "Projetos"],
    "estrategia": ["Estrat√©gias & Funis", "Estrategias", "Funis", "Estrat√©gia"],
    "copy": ["Processo de Copywriting", "Copywriting", "Copy"],
    "social_media": ["Agendamentos & Publica√ß√µes", "Agendamentos", "Publica√ß√µes", "Social Media", "Social"],
    "design": ["Design & Cria√ß√£o", "Design", "Cria√ß√£o", "Criacao"],
    "edicao_de_videos": ["Grava√ß√£o & Edi√ß√£o", "Gravacao", "Edicao", "Edi√ß√£o", "Videos", "V√≠deos"],
    "trafego": ["Gest√£o de Campanhas", "Campanhas", "Tr√°fego", "Trafego", "Ads", "Performance"],
    "infra_automacoes": ["Processos de Automa√ß√µes", "Automacoes", "Automa√ß√µes", "Desenvolvimento Web", "Landing Page", "Manuten√ß√£o & Atualiza√ß√µes"],
    "comercial": ["Comercial", "Vendas"],
    "suporte": ["Suporte", "Atendimento"],
    "checkpoints": ["checkpoints", "Checkpoints", "Check-points"]
}

AREAS_CANON = list(AREA_SYNONYMS.keys())

CACHE_PATH = os.path.join("scripts", ".cache_lists_map.json")
OVERRIDE_PATH = os.path.join("scripts", "lists_map.override.json")

def get(url, params=None):
    for _ in range(5):
        r = requests.get(url, headers=HEAD, params=params, timeout=30)
        if r.status_code in (200, 201):
            return r.json()
        if r.status_code == 429:
            time.sleep(2)
            continue
        r.raise_for_status()
    raise RuntimeError("GET falhou repetidamente")

def find_space_id_by_name(team_id, name_like):
    data = get(f"{API}/team/{team_id}/space")
    for sp in data.get("spaces", []):
        if sp["name"].strip().lower() == name_like.strip().lower():
            return sp["id"]
    return None

def collect_lists_in_space(space_id):
    lists = []

    # listas no root do space
    try:
        data = get(f"{API}/space/{space_id}/list")
        root_lists = data.get("lists", [])
        lists += root_lists
        print(f"  ‚Üí Listas no root do Space: {len(root_lists)}")
        for lst in root_lists:
            print(f"     ‚Ä¢ '{lst['name']}' (ID: {lst['id']})")
    except Exception as e:
        print(f"  ‚ö†Ô∏è Erro ao buscar listas no root: {e}")

    # listas dentro de pastas
    try:
        fd = get(f"{API}/space/{space_id}/folder")
        folders = fd.get("folders", [])
        print(f"  ‚Üí Pastas encontradas: {len(folders)}")
        for folder in folders:
            print(f"     ‚Ä¢ Pasta: '{folder['name']}' (ID: {folder['id']})")
            ld = get(f"{API}/folder/{folder['id']}/list")
            folder_lists = ld.get("lists", [])
            print(f"       - Listas na pasta: {len(folder_lists)}")
            for lst in folder_lists:
                print(f"         ‚ó¶ '{lst['name']}' (ID: {lst['id']})")
            lists += folder_lists
    except Exception as e:
        print(f"  ‚ö†Ô∏è Erro ao buscar pastas/listas: {e}")
    return lists

def normalize_name(n):
    """Normaliza nome removendo acentos, pontua√ß√£o e convertendo para lowercase"""
    import unicodedata
    n = unicodedata.normalize('NFD', n)
    n = ''.join(c for c in n if unicodedata.category(c) != 'Mn')
    n = n.strip().lower()
    n = re.sub(r"[^\w\s]+", " ", n)
    n = re.sub(r"\s+", " ", n).strip()
    return n

def fuzzy_match(list_name, synonyms):
    """Verifica se o nome da lista corresponde a algum sin√¥nimo"""
    norm_list = normalize_name(list_name)
    for syn in synonyms:
        norm_syn = normalize_name(syn)
        # Match exato
        if norm_list == norm_syn:
            return True
        # Match parcial (cont√©m)
        if norm_syn in norm_list or norm_list in norm_syn:
            return True
    return False

def auto_map_lists(all_lists):
    """Mapeia automaticamente listas para √°reas usando sin√¥nimos"""
    mapping = {area: None for area in AREAS_CANON}
    mapping_details = {area: {"list_id": None, "list_name": None, "matched_synonym": None} for area in AREAS_CANON}

    # Criar um dicion√°rio de listas j√° mapeadas para evitar duplicatas
    used_lists = set()

    for area, synonyms in AREA_SYNONYMS.items():
        for lst in all_lists:
            if lst['id'] in used_lists:
                continue
            if fuzzy_match(lst['name'], synonyms):
                mapping[area] = lst['id']
                mapping_details[area] = {
                    "list_id": lst['id'],
                    "list_name": lst['name'],
                    "matched_synonym": "auto-matched"
                }
                used_lists.add(lst['id'])
                break

    return mapping, mapping_details

def main():
    print("=" * 60)
    print("üîÑ RETROFIT - Mapeamento de Listas EXISTENTES")
    print("=" * 60)

    # Usar SPACE_ID do .env se fornecido, sen√£o buscar por nome
    if SPACE_ID:
        print(f"‚úÖ Usando Space ID do .env: {SPACE_ID}")
        op_space = SPACE_ID
    else:
        print("üîç Buscando Space 'Opera√ß√£o LYL' pelo nome...")
        op_space = find_space_id_by_name(TEAM, "Opera√ß√£o LYL")
        if not op_space:
            print("‚ùå Space 'Opera√ß√£o LYL' n√£o encontrado.")
            sys.exit(1)
        print(f"‚úÖ Space encontrado: {op_space}")

    print("\nüìÇ Coletando listas do Space...")
    all_lists = collect_lists_in_space(op_space)

    if not all_lists:
        print("\n‚ö†Ô∏è NENHUMA lista encontrada no Space!")
        print("Poss√≠veis causas:")
        print("  1. Token sem permiss√£o para ler listas/folders")
        print("  2. Space vazio")
        print("  3. Space ID incorreto")
        print("\nPr√≥ximos passos:")
        print("  - Crie um novo token com permiss√µes: View Spaces, View Folders, View Lists")
        print("  - OU forne√ßa os List IDs manualmente em lists_map.override.json")
        sys.exit(1)

    print(f"\nüìã Total de listas encontradas: {len(all_lists)}")

    # Auto-mapeamento
    print("\nü§ñ Realizando mapeamento autom√°tico com sin√¥nimos...")
    mapping, mapping_details = auto_map_lists(all_lists)

    # Mostrar resultado do mapeamento
    print("\n" + "=" * 60)
    print("üìä RESULTADO DO MAPEAMENTO")
    print("=" * 60)

    mapped = []
    unmapped = []

    for area in AREAS_CANON:
        details = mapping_details[area]
        if details['list_id']:
            mapped.append(area)
            print(f"‚úÖ {area:20} ‚Üí '{details['list_name']}' (ID: {details['list_id']})")
        else:
            unmapped.append(area)
            print(f"‚ùå {area:20} ‚Üí N√ÉO MAPEADA")

    # Salvar cache
    cache_data = {
        "space_id": op_space,
        "generated_at": datetime.now().isoformat(),
        "lists": mapping,
        "details": mapping_details,
        "all_lists_found": [{"id": l['id'], "name": l['name']} for l in all_lists]
    }

    with open(CACHE_PATH, "w", encoding="utf-8") as f:
        json.dump(cache_data, f, ensure_ascii=False, indent=2)
    print(f"\nüíæ Cache salvo em: {CACHE_PATH}")

    # Gerar override se necess√°rio
    if unmapped:
        print("\n" + "=" * 60)
        print("‚ö†Ô∏è LISTAS N√ÉO MAPEADAS")
        print("=" * 60)
        print(f"As seguintes √°reas n√£o foram mapeadas automaticamente:")
        for area in unmapped:
            print(f"  - {area}")
            print(f"    Sin√¥nimos esperados: {', '.join(AREA_SYNONYMS[area])}")

        # Gerar arquivo de override
        override_data = {
            "_comment": "Edite este arquivo para mapear manualmente √°reas para List IDs",
            "_instructions": "Substitua null pelo List ID correto (n√∫mero)",
            "_available_lists": [{"id": l['id'], "name": l['name']} for l in all_lists],
            "mapping": {area: mapping[area] for area in AREAS_CANON}
        }

        with open(OVERRIDE_PATH, "w", encoding="utf-8") as f:
            json.dump(override_data, f, ensure_ascii=False, indent=2)

        print(f"\nüìù Arquivo de override gerado: {OVERRIDE_PATH}")
        print("Edite este arquivo para corrigir mapeamentos manualmente.")
        print("Ap√≥s editar, rode novamente 'make distribute' (ele usar√° o override).")
    else:
        print("\n" + "=" * 60)
        print("‚úÖ SUCESSO - Todas as √°reas foram mapeadas!")
        print("=" * 60)
        print("Pr√≥ximo passo: rodar 'make distribute CSV=/caminho/arquivo.csv --dry-run'")

    print("\n" + "=" * 60)

if __name__ == "__main__":
    main()
